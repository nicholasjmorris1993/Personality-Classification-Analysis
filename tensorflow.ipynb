{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb00f0e-b45a-475a-ab3f-1647e3f87002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Obtaining dependency information for plotly from https://files.pythonhosted.org/packages/a8/07/72953cf70e3bd3a24cbc3e743e6f8539abe6e3e6d83c3c0c83426eaffd39/plotly-5.18.0-py3-none-any.whl.metadata\n",
      "  Using cached plotly-5.18.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly)\n",
      "  Obtaining dependency information for tenacity>=6.2.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from plotly) (23.1)\n",
      "Using cached plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.18.0 tenacity-8.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo\n",
      "Successfully installed py-cpuinfo-9.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas-datareader\n",
      "  Using cached pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Collecting lxml (from pandas-datareader)\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/44/1b/0771c38e65ad23e25368b5e07c920054774b8d12477a4fad116bf500de73/lxml-4.9.3-cp38-cp38-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached lxml-4.9.3-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas-datareader) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas-datareader) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas>=0.23->pandas-datareader) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas>=0.23->pandas-datareader) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from pandas>=0.23->pandas-datareader) (1.24.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->pandas-datareader) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->pandas-datareader) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->pandas-datareader) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from requests>=2.19.0->pandas-datareader) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas-datareader) (1.16.0)\n",
      "Using cached lxml-4.9.3-cp38-cp38-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "Installing collected packages: lxml, pandas-datareader\n",
      "Successfully installed lxml-4.9.3 pandas-datareader-0.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from nltk) (1.3.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/79/33/67c4ed826f5227655225c3feaaecd15afb8453e827334ddae95a7fba07ac/regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Using cached regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.10.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install missing packages\n",
    "%pip install plotly\n",
    "%pip install py-cpuinfo\n",
    "%pip install pandas-datareader\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28211ea4-4ff7-48eb-a395-53442492a061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 15.47 GB\n",
      "Available RAM: 14.08 GB\n",
      "Used RAM: 1.12 GB\n",
      "Percentage Usage Of RAM: 9.0%\n",
      "CPU Cores: 4\n",
      "CPU Speed: 2.5000 GHz\n",
      "Total Disk: 24.99 GB\n",
      "Available Disk: 18.41 GB\n",
      "Used Disk: 6.58 GB\n",
      "Percentage Usage Of Disk: 26.3%\n"
     ]
    }
   ],
   "source": [
    "# check system details\n",
    "import os\n",
    "import psutil\n",
    "import cpuinfo\n",
    "\n",
    "ram_info = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {ram_info.total / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"Available RAM: {ram_info.available / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"Used RAM: {ram_info.used / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"Percentage Usage Of RAM: {ram_info.percent}%\")\n",
    "print(f\"CPU Cores: {os.cpu_count()}\")\n",
    "print(f\"CPU Speed: {cpuinfo.get_cpu_info()['hz_actual_friendly']}\")\n",
    "disk_info = psutil.disk_usage(os.getcwd())\n",
    "print(f\"Total Disk: {disk_info.total / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"Available Disk: {disk_info.free / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"Used Disk: {disk_info.used / 1024 / 1024 / 1024:.2f} GB\")\n",
    "print(f\"Percentage Usage Of Disk: {disk_info.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa0ef51-d46b-4ccf-9caa-741ad90c77c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2023-10-31 14:14:36.360906: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import requirements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from nnet_classifier import Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32da91cf-563e-4206-97d7-6a3c7af2bcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the data\n",
    "mbti = pd.read_csv(\"mbti_labels.csv\")\n",
    "users = pd.read_csv(\"user_info.csv\")\n",
    "tweets = pd.read_csv(\"user_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da8397e-988e-4b50-a481-7d22532aceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns from users\n",
    "users = users.drop(columns=[\"id_str\", \"name\", \"screen_name\", \"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af105cf-0888-4d18-a855-26ff8e68f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only keep the first tweets\n",
    "# tweets = tweets.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f12a06-eb16-4d62-9ea4-e4c1189aa9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge all the tweets together\n",
    "# tweet = pd.DataFrame()\n",
    "# for i in range(tweets.shape[0]):\n",
    "#     Id = tweets[\"id\"][i]\n",
    "#     text = \" \".join(tweets.iloc[i, 1:].astype(str).tolist())\n",
    "#     tweet = pd.concat([\n",
    "#         tweet, \n",
    "#         pd.DataFrame({\"id\": [Id], \"tweet\": [text]}),\n",
    "#     ], axis=\"index\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f87aa6e-8841-4a8a-b45c-32e4d6d0950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the data together\n",
    "mbti[\"id\"] = mbti[\"id\"].astype(str)\n",
    "users[\"id\"] = users[\"id\"].astype(str)\n",
    "# tweet[\"id\"] = tweet[\"id\"].astype(str)\n",
    "personality = mbti.merge(right=users, how=\"left\", on=\"id\")\n",
    "# personality = personality.merge(right=tweet, how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a010638-a2f7-47ba-848e-fbec65b27da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values with None\n",
    "personality = personality.fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6160cd3-b46e-4457-bc75-b4ab946e4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove id\n",
    "personality = personality.drop(columns=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d7764d-4189-48f2-950f-a3a7c65a923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make verified a binary variable\n",
    "personality[\"verified\"] = personality[\"verified\"] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de787e51-c6cc-4482-8a52-04b766744ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "personality = personality.sample(frac=1, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f9301a-5582-4abd-8e1b-33a9302eb898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the testing data\n",
    "y = personality[[\"mbti_personality\"]]\n",
    "X = personality.drop(columns=\"mbti_personality\")\n",
    "testX = X.tail(int(0.2 * X.shape[0])).reset_index(drop=True)\n",
    "testy = y.tail(int(0.2 * y.shape[0])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a543694e-96f5-4e1e-ac1c-17e2694f648b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Personality Classification Analysis ----\n",
      "\n",
      "Visualizing The Data:\n",
      "> Plotting Correlations\n",
      "> total_retweet_count vs. average_retweet_count\n",
      "> total_favorite_count vs. average_favorite_count\n",
      "> total_hashtag_count vs. average_hashtag_count\n",
      "> total_url_count vs. average_url_count\n",
      "> total_mentions_count vs. average_mentions_count\n",
      "> total_media_count vs. average_media_count\n",
      "> Plotting followers_count\n",
      "> Plotting friends_count\n",
      "> Plotting listed_count\n",
      "> Plotting favourites_count\n",
      "> Plotting statuses_count\n",
      "> Plotting number_of_quoted_statuses\n",
      "> Plotting number_of_retweeted_statuses\n",
      "> Plotting total_retweet_count\n",
      "> Plotting total_favorite_count\n",
      "> Plotting total_hashtag_count\n",
      "> Plotting total_url_count\n",
      "> Plotting total_mentions_count\n",
      "> Plotting total_media_count\n",
      "> Plotting number_of_tweets_scraped\n",
      "> Plotting average_tweet_length\n",
      "> Plotting average_retweet_count\n",
      "> Plotting average_favorite_count\n",
      "> Plotting average_hashtag_count\n",
      "> Plotting average_url_count\n",
      "> Plotting average_mentions_count\n",
      "> Plotting average_media_count\n",
      "> Plotting verified\n",
      "> mbti_personality vs. verified\n",
      "> followers_count vs. mbti_personality\n",
      "> followers_count vs. verified\n",
      "> friends_count vs. mbti_personality\n",
      "> friends_count vs. verified\n",
      "> listed_count vs. mbti_personality\n",
      "> listed_count vs. verified\n",
      "> favourites_count vs. mbti_personality\n",
      "> favourites_count vs. verified\n",
      "> statuses_count vs. mbti_personality\n",
      "> number_of_quoted_statuses vs. mbti_personality\n",
      "> number_of_quoted_statuses vs. verified\n",
      "> number_of_retweeted_statuses vs. mbti_personality\n",
      "> total_retweet_count vs. mbti_personality\n",
      "> total_retweet_count vs. verified\n",
      "> total_favorite_count vs. mbti_personality\n",
      "> total_favorite_count vs. verified\n",
      "> total_hashtag_count vs. mbti_personality\n",
      "> total_hashtag_count vs. verified\n",
      "> total_url_count vs. mbti_personality\n",
      "> total_url_count vs. verified\n",
      "> total_mentions_count vs. mbti_personality\n",
      "> total_mentions_count vs. verified\n",
      "> total_media_count vs. mbti_personality\n",
      "> total_media_count vs. verified\n",
      "> average_tweet_length vs. mbti_personality\n",
      "> average_retweet_count vs. mbti_personality\n",
      "> average_retweet_count vs. verified\n",
      "> average_favorite_count vs. mbti_personality\n",
      "> average_favorite_count vs. verified\n",
      "> average_hashtag_count vs. mbti_personality\n",
      "> average_hashtag_count vs. verified\n",
      "> average_url_count vs. mbti_personality\n",
      "> average_url_count vs. verified\n",
      "> average_mentions_count vs. mbti_personality\n",
      "> average_mentions_count vs. verified\n",
      "> average_media_count vs. mbti_personality\n",
      "> average_media_count vs. verified\n",
      "9.3 Seconds\n",
      "Model Training:\n",
      "> Transforming The Training Data\n",
      "> Renaming Features\n",
      "> Transforming Text Features\n",
      "> Renaming Features\n",
      "> Removing Constant Features\n",
      "> Selecting Features\n",
      "> Scaling Features\n",
      "> Training Neural Network\n",
      "48.78 Minutes\n",
      "Model Performance:\n",
      "> Transforming The Testing Data\n",
      "> Scoring The Model\n",
      "7.66 Seconds\n",
      "Model Indicators:\n",
      "> Perturbing Features\n",
      "4.87 Minutes\n",
      "Model Prediction:\n",
      "> Transforming The New Data\n",
      "> Getting Predictions\n",
      "1.91 Seconds\n",
      "Model Monitoring:\n",
      "> Computing Feature Drift\n",
      "12.66 Seconds\n",
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.16470673076923079\n",
      "F1: 0.1125656145887394\n",
      "In Control: 100.0%\n",
      "Confusion Matrix:\n",
      "          enfj      enfp      entj      entp      esfj      esfp      estj      estp      infj      infp      intj      intp      isfj      isfp      istj      istp\n",
      "enfj  0.073874  0.001802  0.000000  0.000601  0.002402  0.000000  0.001201  0.000000  0.001201  0.003003  0.000000  0.000000  0.000000  0.000000  0.002402  0.001802\n",
      "enfp  0.003003  0.082883  0.000000  0.000000  0.003003  0.000000  0.001802  0.000601  0.000000  0.001802  0.000000  0.000601  0.001201  0.000601  0.001201  0.002402\n",
      "entj  0.003604  0.000000  0.000601  0.001201  0.000601  0.001201  0.000601  0.000601  0.068468  0.002402  0.000000  0.000000  0.000601  0.000000  0.000000  0.001201\n",
      "entp  0.001802  0.001201  0.000601  0.002402  0.000601  0.001201  0.000601  0.001802  0.000000  0.063664  0.000000  0.000601  0.000601  0.001201  0.001201  0.001201\n",
      "esfj  0.001802  0.000601  0.001201  0.000000  0.001201  0.000601  0.000601  0.000000  0.000000  0.000000  0.024625  0.000601  0.000000  0.000000  0.001201  0.000601\n",
      "esfp  0.000000  0.000000  0.000000  0.000000  0.000601  0.000000  0.000601  0.000000  0.000000  0.000000  0.002402  0.015015  0.000000  0.000000  0.000601  0.000000\n",
      "estj  0.000601  0.000000  0.000000  0.000601  0.000601  0.000601  0.000000  0.000000  0.000601  0.000601  0.000000  0.000000  0.018619  0.000601  0.000000  0.000000\n",
      "estp  0.000000  0.000000  0.000601  0.000000  0.002402  0.003604  0.001802  0.000000  0.000601  0.000601  0.000000  0.000000  0.000000  0.010210  0.000601  0.001201\n",
      "infj  0.001802  0.000000  0.001201  0.001802  0.001802  0.000601  0.000601  0.000000  0.001201  0.000601  0.001201  0.001201  0.000601  0.000000  0.092492  0.004805\n",
      "infp  0.003604  0.001802  0.001201  0.000601  0.001201  0.000000  0.000000  0.000000  0.001201  0.001201  0.000000  0.000601  0.000601  0.000000  0.003604  0.093694\n",
      "intj  0.000601  0.001201  0.096096  0.000000  0.001201  0.000601  0.000601  0.000000  0.001201  0.000601  0.000000  0.000000  0.000601  0.000000  0.001802  0.001201\n",
      "intp  0.001802  0.000000  0.000000  0.078078  0.004204  0.001201  0.000601  0.000000  0.000000  0.002402  0.000000  0.000000  0.000000  0.000000  0.000000  0.001802\n",
      "isfj  0.000601  0.000601  0.000000  0.000601  0.045045  0.001802  0.001802  0.001802  0.000601  0.000000  0.000601  0.000000  0.001201  0.000601  0.000601  0.000601\n",
      "isfp  0.000601  0.000000  0.000000  0.000601  0.001201  0.018018  0.000000  0.001201  0.000000  0.001201  0.000000  0.001201  0.000601  0.000000  0.001201  0.001201\n",
      "istj  0.000000  0.000000  0.000601  0.000000  0.001201  0.001201  0.028228  0.000601  0.000601  0.001201  0.000000  0.001201  0.001201  0.000601  0.000601  0.000000\n",
      "istp  0.001201  0.000000  0.000000  0.000601  0.000601  0.000601  0.000000  0.015616  0.000000  0.000000  0.000601  0.000000  0.000000  0.000601  0.000601  0.000000\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "print(\"\\n---- Personality Classification Analysis ----\\n\")\n",
    "model = Classification(\n",
    "    name=\"Tensorflow Without Feature Engineering\", \n",
    "    path=None,\n",
    "    rename=True, \n",
    "    time=False, \n",
    "    text=True,\n",
    "    binary=False, \n",
    "    imputation=False, \n",
    "    variance=True,\n",
    "    scale=True,\n",
    "    atwood=False,\n",
    "    binning=False,\n",
    "    reciprocal=False, \n",
    "    interaction=False, \n",
    "    selection=True,\n",
    "    tune=False,\n",
    "    plots=True,\n",
    ")\n",
    "try:\n",
    "    model.load()  # load the machine learning pipeline\n",
    "    predictions = model.predict(testX)\n",
    "except:\n",
    "    model.explore(personality)\n",
    "    model.validate(X, y)  # build the machine learning pipeline\n",
    "    predictions = model.predict(testX)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Accuracy: {model.accuracy}\")\n",
    "    print(f\"F1: {model.f1}\")\n",
    "    print(f\"In Control: {model.in_control}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "    print(model.confusion / model.confusion.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b88af05-8286-459b-b34d-0086c4065fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Indicators:\n",
      "1. total_retweet_count\n",
      "2. total_media_count\n",
      "3. average_media_count\n",
      "4. total_mentions_count\n",
      "5. description_positivity1\n",
      "6. total_hashtag_count\n",
      "7. average_retweet_count\n",
      "8. total_favorite_count\n",
      "9. friends_count\n",
      "10. average_favorite_count\n",
      " \n",
      "Feature Drift:\n",
      "1. average_media_count\n",
      "2. mbti_personality\n"
     ]
    }
   ],
   "source": [
    "# model diagnostics\n",
    "print(\"Model Indicators:\")\n",
    "for i, indicator in enumerate(model.indicators[\"Indicator\"][:10].tolist()):\n",
    "    print(f\"{i+1}. {indicator}\")\n",
    "print(\" \")\n",
    "print(\"Feature Drift:\")\n",
    "for i, feature in enumerate(model.drift.loc[model.drift[\"pvalue\"] < 0.05, \"Feature\"][:10].tolist()):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "if model.drift.loc[model.drift[\"pvalue\"] < 0.05].shape[0] == 0:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b9ca50-06d6-49b7-848b-c0852e302909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16516516516516516\n",
      "F1: 0.1128278526105363\n"
     ]
    }
   ],
   "source": [
    "# score the model\n",
    "accuracy = accuracy_score(\n",
    "    y_true=testy.iloc[:,0].to_numpy(),\n",
    "    y_pred=predictions,\n",
    ")\n",
    "f1 = f1_score(\n",
    "    y_true=testy.iloc[:,0].to_numpy(),\n",
    "    y_pred=predictions,\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da46103b-b661-4234-8a31-9804f9139435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "          enfj      enfp      entj      entp      esfj      esfp      estj      estp      infj      infp      intj      intp      isfj      isfp      istj      istp\n",
      "enfj  0.073874  0.001802  0.000000  0.000601  0.002402  0.000000  0.001201  0.000000  0.001201  0.003003  0.000000  0.000000  0.000000  0.000000  0.002402  0.001802\n",
      "enfp  0.003003  0.082883  0.000000  0.000000  0.003003  0.000000  0.001802  0.000601  0.000000  0.001802  0.000000  0.000601  0.001201  0.000601  0.001201  0.002402\n",
      "entj  0.003604  0.000000  0.000601  0.001201  0.000601  0.001201  0.000601  0.000601  0.068468  0.002402  0.000000  0.000000  0.000601  0.000000  0.000000  0.001201\n",
      "entp  0.001802  0.001201  0.000601  0.002402  0.000601  0.001201  0.000601  0.001802  0.000000  0.063664  0.000000  0.000601  0.000601  0.001201  0.001201  0.001201\n",
      "esfj  0.001802  0.000601  0.001201  0.000000  0.001201  0.000601  0.000601  0.000000  0.000000  0.000000  0.024625  0.000601  0.000000  0.000000  0.001201  0.000601\n",
      "esfp  0.000000  0.000000  0.000000  0.000000  0.000601  0.000000  0.000601  0.000000  0.000000  0.000000  0.002402  0.015015  0.000000  0.000000  0.000601  0.000000\n",
      "estj  0.000601  0.000000  0.000000  0.000601  0.000601  0.000601  0.000000  0.000000  0.000601  0.000601  0.000000  0.000000  0.018619  0.000601  0.000000  0.000000\n",
      "estp  0.000000  0.000000  0.000601  0.000000  0.002402  0.003604  0.001802  0.000000  0.000601  0.000601  0.000000  0.000000  0.000000  0.010210  0.000601  0.001201\n",
      "infj  0.001802  0.000000  0.001201  0.001802  0.001802  0.000601  0.000601  0.000000  0.001201  0.000601  0.001201  0.001201  0.000601  0.000000  0.092492  0.004805\n",
      "infp  0.003604  0.001802  0.001201  0.000601  0.001201  0.000000  0.000000  0.000000  0.001201  0.001201  0.000000  0.000601  0.000601  0.000000  0.003604  0.093694\n",
      "intj  0.000601  0.001201  0.096096  0.000000  0.001201  0.000601  0.000601  0.000000  0.001201  0.000601  0.000000  0.000000  0.000601  0.000000  0.001802  0.001201\n",
      "intp  0.001802  0.000000  0.000000  0.078078  0.004204  0.001201  0.000601  0.000000  0.000000  0.002402  0.000000  0.000000  0.000000  0.000000  0.000000  0.001802\n",
      "isfj  0.000601  0.000601  0.000000  0.000601  0.045045  0.001802  0.001802  0.001802  0.000601  0.000000  0.000601  0.000000  0.001201  0.000601  0.000601  0.000601\n",
      "isfp  0.000601  0.000000  0.000000  0.000601  0.001201  0.018018  0.000000  0.001201  0.000000  0.001201  0.000000  0.001201  0.000601  0.000000  0.001201  0.001201\n",
      "istj  0.000000  0.000000  0.000601  0.000000  0.001201  0.001201  0.028228  0.000601  0.000601  0.001201  0.000000  0.001201  0.001201  0.000601  0.000601  0.000000\n",
      "istp  0.001201  0.000000  0.000000  0.000601  0.000601  0.000601  0.000000  0.015616  0.000000  0.000000  0.000601  0.000000  0.000000  0.000601  0.000601  0.000000\n"
     ]
    }
   ],
   "source": [
    "# show the confusion matrix\n",
    "ytest = testy.iloc[:,0].to_numpy()\n",
    "labels = np.unique(np.concatenate((predictions, ytest)))\n",
    "confusion = confusion_matrix(\n",
    "    y_true=ytest,   # rows\n",
    "    y_pred=predictions,  # columns\n",
    "    labels=labels,\n",
    ")\n",
    "confusion = pd.DataFrame(\n",
    "    confusion, \n",
    "    columns=labels, \n",
    "    index=labels,\n",
    ")\n",
    "print(\"Confusion Matrix:\")\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "print(confusion / confusion.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2226e286-b762-4265-a3de-de9a5561d5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the machine learning pipeline\n",
    "model.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd600fc-dae1-4f58-9117-0ce0d7fc232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Retraining:\n",
      "> Transforming The Updated Data\n",
      "> Renaming Features\n",
      "> Transforming Text Features\n",
      "> Renaming Features\n",
      "> Removing Constant Features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# refit the model to include the test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesty\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sagemaker-studiolab-notebooks/personality-classification/nnet_classifier.py:415\u001b[0m, in \u001b[0;36mClassification.refit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    413\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames2\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m    414\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpute\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m--> 415\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselection1\u001b[38;5;241m.\u001b[39mfit_transform(X, y)\n\u001b[1;32m    417\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler1\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "File \u001b[0;32m~/sagemaker-studiolab-notebooks/personality-classification/nnet_classifier.py:1436\u001b[0m, in \u001b[0;36mConstantFeatures.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m-> 1436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sagemaker-studiolab-notebooks/personality-classification/nnet_classifier.py:1430\u001b[0m, in \u001b[0;36mConstantFeatures.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance:\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[0;32m-> 1430\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/sklearn/feature_selection/_base.py:96\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m     89\u001b[0m     X,\n\u001b[1;32m     90\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/sklearn/feature_selection/_base.py:112\u001b[0m, in \u001b[0;36mSelectorMixin._transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mreshape((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/sklearn/utils/__init__.py:355\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages/sklearn/utils/__init__.py:184\u001b[0m, in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    183\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array[key] \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# refit the model to include the test data\n",
    "model.refit(testX, testy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
